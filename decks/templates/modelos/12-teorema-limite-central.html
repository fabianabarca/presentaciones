{% extends 'decks.html' %}

{% load static %}

<!-- Título -->
{% block title %}12-teorema-limite-central{% endblock %}

<!-- Referencia: https://www.overleaf.com/project/5c376b1f3d7cdc5c9060a1da  -->

<!-- Configuraciones específicas del <head> -->
{% block head %}
<script src="https://cdn.jsdelivr.net/npm/chart.js"></script>

<!-- Agregar KaTex de reveal Revealjs -->
<script src="plugin/math/math.js"></script>
<script>
  Reveal.initialize({ plugins: [ RevealMath.KaTeX ] });
</script>
{% endblock %}
{% block content %}

<!-- SECCIÓN 0 -->
<section>
    <h1 class="display-1">
        Teorema del límite central
    </h1>
    <h2 class="display-1">
        Desigualdades de Chebyshev y Markov y ley de los grandes números
    </h2>
</section>


<!-- Portada y descripción -->
<section>
    <p>
        El teorema del límite central establece que la función de distribución probabilística de la <strong>suma</strong> o de las <strong>medias muestrales</strong> de un número grande de variables aleatorias aproxima a una <strong>distribución gaussiana</strong> (normal).
    </p>
</section>

<!-- Portada y descripción 2 -->
<section>
    <p>
        El teorema es clave en la teoría de probabilidad porque implica que los métodos probabilísticos y estadísticos que funcionan para distribuciones <strong>normales</strong> pueden ser aplicables a muchos problemas que involucran <strong>otros tipos de distribuciones</strong>.
    </p>
    <p>
        También ayuda a explicar por qué la distribución normal es tan&hellip; normal.
    </p>
</section>

<!-- Teorema del límite central para sumas -->
<section>
    <section>
        <h2>Teorema del límite central para sumas</h2>
    </section>
  
    <!-- Diapositiva 1 -->
    <section>
        <h3>El teorema del límite central para sumas</h3>
    
        <blockquote>
        <strong>Definición de la convergencia en la distribución</strong>
        <p>
            Considérese \( X_1, \ldots, X_N \) variables aleatorias <em>independientes e idénticamente distribuidas</em> <strong>IID</strong>, con media común \( \mu_{X_i} = \mu \) y desviación estándar \( \sigma_{X_i} = \sigma \). Sea además:
        </p>
    
        <p>
            \[
            S_N = X_1 + \cdots + X_N
            \]
        </p>
    
        <p>
            la <strong>suma</strong> de ellas. Según el teorema del límite central, \( S_N \sim \mathcal{N}(N \mu, N \sigma^2) \) conforme \( N \to \infty \). Alternativamente, si \( Z \) es una distribución <em>normalizada</em> de \( S_N \),
        </p>
    
        <p>
            \[
            Z = \frac{S_N - \mu_{S_N}}{\sigma_{S_N}} = \frac{S_N - N\mu}{\sigma \sqrt{N}}
            \]
        </p>
    
        <p>\( Z \) tiene una distribución \( f_Z(z) \) que aproxima a \( \mathcal{N}(0,1) \) conforme \( N \to \infty \).</p>
        </blockquote>
    
        <p><small>\( \mathcal{N}(0,1) \) es una función de distribución gaussiana de media 0 y varianza 1, donde \( \mathcal{N} \) alude a "normal".</small></p>
    </section>
  
    <!-- Diapositiva 2 -->
    <section>
        <p> Figura (Falta)</p>
    </section>

    <!-- Diapositiva 3 -->
    <section>
        <p> Figura (Falta)</p>
    </section>

    <!-- Diapositiva 4 -->
    <section>
        <h3>Ejemplo de los resistores en serie I, Interpretación del teorema del límite central para sumas</h3>
    
        <p>Los resistores tienen una resistencia <em>nominal</em> y un porcentaje de <em>tolerancia</em>. Por ejemplo, un resistor de 330 ohm con una tolerancia del 5 % se espera que tenga una resistencia entre 313.5 y 346.5 ohm.</p>
    
        <blockquote>
        <p>
            Considérense cinco resistores de 330 ohm, escogidos aleatoriamente de una población con 5 % de tolerancia, y modélese la resistencia de cada uno como una distribución uniforme en \( [313.5, 346.5] \). Si son conectados en serie, ¿cuál es la distribución de la resistencia \( R \) del sistema, dada por \( R = X_1 + \ldots + X_5 \)? Los \( X_i \) son los valores de resistencia IDD.
        </p>
        </blockquote>
    </section>
    
    <!-- Diapositiva 5 -->
    <section>
        <h3>Ejemplo de los resistores en serie II, Interpretación del teorema del límite central para sumas</h3>
    
        <p>Una variable aleatoria uniformemente distribuida en \( [a, b] \) tiene media \( \mu = \frac{a + b}{2} \) y desviación estándar \( \sigma = \frac{b - a}{\sqrt{12}} \). Para <strong>cada resistencia</strong>:</p>
    
        <ul>
        <li>la media es \( E[X_i] = \frac{313.5 + 346.5}{2} = 330~\Omega \), es decir, la resistencia nominal</li>
        <li>la desviación estándar es \( \sigma = \frac{346.5 - 313.5}{\sqrt{12}} = 9.529~\Omega \)</li>
        </ul> 
    
        <p>La resistencia <strong>del sistema en serie</strong> tiene una media y desviación estándar de:</p>
    
        <p>
        \[
        \begin{aligned}
        E[R] 	&= N\mu = 5 \cdot 330 = 1650~\Omega \\
        SD[R] 	&= \sqrt{N} \sigma = \sqrt{5} \cdot 9.529 = 21.3~\Omega
        \end{aligned}
        \]
        </p>
    </section>

    <!-- Diapositiva 6 -->
    <section>
        <h3>Ejemplo de los resistores en serie III, Interpretación del teorema del límite central para sumas</h3>
    
        <p>¿Cómo es la distribución de probabilidad de \( R = X_1 + \ldots + X_5 \)? ¿Es \( R \) también distribuido uniformemente? Una simulación de 10,000 instancias distintas de \( R \) muestra:</p>
        
        <p> Figura (Falta)</p>

        <p>que es una muy buena aproximación de \( \mathcal{N}(\mu,\sigma) = \mathcal{N}(1650,21.3) \).</p>
    </section>

    <!-- Diapositiva 7 -->
    <section>
        <h3>Ejemplo de la revisión de formularios antes del mediodía I</h3>
    
        <blockquote>
        <p>
            Hay 40 formularios por revisar. Por los años de experiencia, la persona que los revisa sabe que el tiempo requerido para revisar cada uno es una variable aleatoria con un valor esperado de 6 minutos y una desviación estándar de 6 minutos. Si los tiempos de revisión son independientes y la persona inicia a las 7:50 a.m. revisando de forma continua, ¿cuál es la probabilidad de que termine antes de las 12:00 m.d.?
        </p>
        </blockquote>
    
        <div class="row">
        <div class="col-md-6">
            <ul>
            <li>Recordar que \( Z = \frac{S_N - N\mu}{\sigma \sqrt{N}} \)</li>
            <li>También que \( f_Z(z) \rightarrow \mathcal{N}(0,1) \)</li>
            <li>¿Cuánto es \( N \)?</li>
            </ul>
        </div>
        <div class="col-md-6">
            <ul>
            <li>¿Cuánto es \( \mu \)?</li>
            <li>¿Cuánto es \( \sigma \)?</li>
            <li>¿Cuánto es el tiempo disponible?</li>
            </ul>
        </div>
        </div>
    </section>
  
  
    <!-- Diapositiva 8 -->
    <section>
        <h3>Ejemplo de la revisión de formularios antes del mediodía II</h3>
    
        <p>Para este problema, se debe aplicar el teorema del límite central para sumas, puesto que se trabajará con la suma del tiempo de revisión de todos los formularios. De la información del enunciado:</p>
    
        <ul>
        <li>40 formularios: \( N = 40 \)</li>
        <li>Tiempo promedio de revisión por formulario: \( \mu = 6 \)</li>
        <li>Desviación estándar: \( \sigma = 6 \)</li>
        </ul>
    
        <p>
        La persona comienza a las 7:50 a.m. y para terminar antes de las 12:00 m.d. deberá hacerlo en menos de 250 minutos. El objetivo es, por tanto, encontrar \( P(S_N \leq 250) \), o de forma equivalente, \( P(Z \leq Z_0) \), donde:
        </p>
    
        <p>
        \[
        Z_0 = \frac{S_0 - N \mu}{\sigma \sqrt{N}} = \frac{250 - 40 \cdot 6}{6 \sqrt{40}} = 0.263
        \]
        </p>
    </section>

    <!-- Diapositiva 7 -->
    <section>
        <h3>Ejemplo de la revisión de formularios antes del mediodía III</h3>

        <p>Es decir,</p>

        <p>
            \[
            P \left( \frac{S_N-40\cdot6}{6 \sqrt{40}} \leq \frac{250-40\cdot6}{6 \sqrt{40}} \right)
            \]
        </p>

        <p>
            \[
            P \left( Z \leq 0.263 \right)
            \]
        </p>

        <p>Utilizando la tabla para probabilidades acumulativas para valores positivos de \( Z \), el valor más cercano a \( 0.263 \) está dado para \( 0.26 \).</p>

        <blockquote>
            \[
            P \left( Z \leq 0.263 \right) \approx P \left( Z \leq 0.26 \right) = 0.6026 \Rightarrow 60.26 \text{%}
            \]
            <p>La persona que revisa tiene un \( 60.26\% \) de probabilidad de completar la revisión de formularios antes del mediodía.</p>
        </blockquote>
    </section>  
</section>

<!-- Teorema del límite central para medias de muestras -->
<section>
    <section>
        <h2>Teorema del límite central para medias de muestras</h2>
     </section>

    <!-- Diapositiva 1: El teorema del límite central para medias de muestras -->
    <section>
        <h3>El teorema del límite central para medias de muestras, Definición de la convergencia en la media</h3>
        <p>
        Si \( \{ X_i \}_{i=1}^N \) es una <strong>muestra</strong> de \( N \) elementos de una <strong>población</strong>, su <strong>media muestral</strong> es \( \overline{X_N} = S_N/N = \mu_N \), donde \( S_N = X_1 + X_2 + \cdots + X_N \). Se puede considerar <strong>Aqui falta la ref</strong> para hacer:
        </p>
    
        <p>
        \[
        \frac{S_N - N\mu}{\sigma \sqrt{N}} \cdot \frac{1/N}{1/N} = \frac{\overline{X_N} - \mu}{\sigma / \sqrt{N}}
        \]
        </p>
    
        <blockquote>
        <strong>Definición de convergencia en la media</strong>
        <p>Sean \( X_1, \ldots, X_N \) una muestra de variables aleatorias <strong>IID</strong>. de una <strong>población</strong> con media común \( \mu \) y desviación estándar \( \sigma \) y con una <strong>media de la muestra</strong> \( \overline{X_N} \). Entonces:</p>
        
        <p>
            \[
            Z = \frac{\overline{X_N} - \mu}{\sigma / \sqrt{N}}
            \]
        </p>
    
        <p>aproxima \( \mathcal{N}(0,1) \) conforme \( N \rightarrow \infty \). Equivalentemente, \( \overline{X_N} \sim \mathcal{N}(\mu, \sigma^2/N) \).</p>
        </blockquote>
    </section>
    
    <!-- Diapositiva 2: Visualización del teorema del límite central para medias de muestras -->
    
    <section>
        <h3>Visualización del teorema del límite central para medias de muestras I</h3>
    
        <p>Aunque una <strong>población</strong> tenga una distribución con media \( \mu = E[X] \), una <strong>realización</strong> o <strong>muestra</strong> de esta distribución tendrá casi siempre un valor ligeramente distinto.</p>
    
        <strong>Ejemplo de una distribución uniforme</strong>
        <p>Sea \( X \sim \mathsf{unif}(0,1) \) con \( \mu = E[X] = \mathbf{0.5} \). Sea además \( X_i \) <strong>una muestra</strong> de esta distribución con 500 elementos y con una media estadística de \( \mu_{X_i} = 0.5138 \neq 0.5 \).</p>
    
        <p>Figura (Falta)</p>
    </section>
    
    <!-- Diapositiva 3: Distribución de medias en la muestra -->
    
    <section>
        <h3>Visualización del teorema del límite central para medias de muestras II</h3>
    
        <p>Al hacer una simulación de \( N \) muestras se obtienen \( N \) medias distintas \( \mu_{X_i} \). ¿Cómo se distribuyen estos valores alrededor de \( \mu \) y cómo cambia la distribución según \( N \)?</p>
        
        <p>Figura (Falta)</p>
        <blockquote>
            <p>
              Entre más grande es \( N \), más "agrupados" están los valores de la media de la muestra \( \overline{X_N} = \mu_N \) alrededor de la "media verdadera" de la población, \( \mu \).
            </p>
        </blockquote>
          
    </section>
    
    <!-- Diapositiva 4: Figura (Falta) -->
    
    <section>
        <h3>Figura (Falta)</h3>
        <p>Figura (Falta)</p>
    </section>
    
    <!-- Diapositiva 5: Ejemplo del número de visitas mensuales al cajero automático -->
    
    <section>
        <h3>Ejemplo del número de visitas mensuales al cajero automático I</h3>
        <p>
            Suponga que el número de veces que un cliente utiliza el cajero automático de un banco en un mes es una variable aleatoria con un valor medio de 3.2 y una desviación estándar de 2.4. El banco conoce estos datos con exactitud pues puede monitorear cada visita de la <strong>población</strong> de sus miles de clientes.
        </p>
        <blockquote>
       
        <p>Si se selecciona aleatoriamente una muestra de 100 clientes, ¿qué tan probable es que el promedio de veces que el cajero es utilizado en la <strong>muestra</strong> exceda 3.5?</p>
        </blockquote>
    </section>
    
    <!-- Diapositiva 6: Ejemplo del número de visitas mensuales al cajero automático II -->
    
    <section>
        <h3>Ejemplo del número de visitas mensuales al cajero automático II</h3>
    
        <p>
        La probabilidad solicitada es \( P(\overline{X_N} > 3.5) \), donde \( \overline{X_N} \) es el valor medio de la muestra. La muestra es grande (\( N = 100 \) clientes) y por tanto la distribución de \( \overline{X_N} \) se puede aproximar a una distribución normal.
        </p>
    
        <p>
        \[
        P(\overline{X_N} > 3.5) \approx P \left( Z > \frac{3.5 - \mu}{\sigma/\sqrt{N}} \right) = P \left( Z > \frac{3.5 - 3.2}{0.24} \right) = 1 - F_{Z}(1.25) = 0.1056
        \]
        </p>
    
        <p>
        La probabilidad es pequeña porque la muestra es grande y la desviación estándar de la muestra es muy pequeña, de solo 0.24, de forma tal que la media de una muestra de 100 personas se acerca "bastante" a la media de la población de quizá miles de personas.
        </p>
    </section>
</section>

<!-- Desigualdades o límites superiores de probabilidad -->
 <section>
    <section>
        <h2>Desigualdades o límites superiores de probabilidad</h2>
    </section>

    <sectioN>
        <p>
            Sin conocer exactamente la distribución de un
            fenómeno aleatorio, es posible establecer estimaciones
            de la probabilidad en un rango conociendo solo su
            varianza o media.
        </p>  
    </sectioN>
    
 </section>

<!-- Desigualdad de Chebyshev -->
<section>
    <section>
        <h2>Desigualdad de Chebyshev</h2>
    </section>

    <!-- Diapositiva 1  -->
    <section>
        <h3>Premisas para la desigualdad de Chebyshev</h3>
    
        <p>Sea \( W \) una variable aleatoria con media 0. Esta es <strong>cualquier</strong> variable aleatoria.</p>
    
        <p>Figura (Falta)</p>
    
        <div class="row">
        <div class="col-md-6">
            <ul>
            <li>La media de \( W \) es 0, pero cualquier realización simple de \( W \) puede estar bastante alejada de 0.</li>
            <li>La varianza es una medida de la dispersión de los valores de \( W \) alrededor de 0.</li>
            </ul>
        </div>
        <div class="col-md-6">
            <ul>
            <li>Entre mayor el valor de la varianza de \( W \), más probable es que el valor de \( W \) esté lejos de 0.</li>
            </ul>
        </div>
        </div>
    
        <blockquote>
            <p>
                Dada la varianza \( \sigma^2 \), ¿qué tan cercanos a \( \mu = 0 \) los valores de \( W \) podrían estar?
            </p>            
        </blockquote>
    </section>
  
    <!-- Diapositiva 2 -->
    
    <section>
        <h3>La desigualdad de Chebyshev I</h3>
    
        <p>Fíjese un número \( \epsilon > 0 \) y búsquese la probabilidad de que \( W \) esté más alejada que \( \epsilon \) de su media \( \mu = 0 \). Supóngase que \( W \) tiene una función de densidad \( f_W(w) \), entonces:</p>
    
        <p>
        \[
        P(\vert W \vert \geq \epsilon ) = \int_{\vert w \vert \geq \epsilon}f_W(w) \, \mathsf{d}w = \int_{w^2 \geq \epsilon^2} \frac{\epsilon^2}{\epsilon^2}f_W(w) \, \mathsf{d}w
        \]
        </p>
    
        <p>Figura (Falta)</p>
    </section>
  
    <!-- Diapositiva 3 -->
    <section>
        <h3>La desigualdad de Chebyshev II</h3>
    
        <p>Es esperable que la probabilidad \( P(\vert W \vert \geq \epsilon) \) debería hacerse más grande conforme \( \sigma^2 \) se hace más grande, puesto que los valores de \( W \) están más dispersos.</p>
    
        <p>
        \[
        \begin{aligned}
        \int_{w^2 \geq \epsilon^2} \frac{\epsilon^2}{\epsilon^2}f_W(w) \, \mathsf{d}w & \leq \int_{w^2 \geq \epsilon^2}\frac{w^2}{\epsilon^2}f_W(w) \, \mathsf{d}w \\
            & \leq \int_{-\infty}^{\infty}\frac{w^2}{\epsilon^2}f_W(w) \, \mathsf{d}w \\
            & = \frac{1}{\epsilon^2}\int_{-\infty}^{\infty} w^2 f_W(w) \, \mathsf{d}w \\
            & = \frac{E[W^2]}{\epsilon^2} \\
        P(\vert W \vert \geq \epsilon) & \leq \frac{\sigma^2}{\epsilon^2}
        \end{aligned}
        \]
        </p>
    </section>
  
    <!-- Diapositiva 4 -->
    <section>
        <h3>La desigualdad de Chebyshev III</h3>
    
        <ul>
        <li>La primera desigualdad es porque el intervalo de integración contiene los puntos \( w \) donde \( w^2 \geq \epsilon^2 \) y, por lo tanto, el integrando será mayor si \( \epsilon^2 \) se reemplaza por \( w^2 \).</li>
        <li>La segunda desigualdad viene de aumentar el intervalo de integración de los puntos \( w \) donde \( w^2 \geq \epsilon^2 \) a la recta numérica de \( -\infty \) a \( +\infty \).</li>
        <li>\( E[W^2] \) (el segundo momento ordinario) es igual en este caso a la varianza \( \sigma^2 \) porque la media es cero y \( \sigma^2 = E[W^2] - E^2[W] \).</li>
        </ul>
    
        <blockquote>
        <strong>Desigualdad de Chebyshev</strong>
        <p>
            Si \( E[W] = 0 \) y dado cualquier número positivo \( \epsilon \), el evento que \( W \) difiera en por lo menos \( \epsilon \) de cero, está acotado por la probabilidad:
        </p>
        \[
        P(\vert W \vert \geq \epsilon ) \leq \frac{\sigma^2}{\epsilon^2}
        \]
        </blockquote>
    </section>
  
  <!-- Diapositiva 5 -->
  <section>
    <h3>Generalización de la desigualdad de Chebyshev</h3>
  
    <p>Si \( \mu = E[X] \neq 0 \) pero \( W = X - \mu \), entonces \( E[W] = 0 \), y el desarrollo anterior aplica a \( X \):</p>
  
    <blockquote>
      <strong>Desigualdad de Chebyshev generalizada</strong>
      <p>
        Sea \( X \) una variable aleatoria con media finita \( \mu \) y varianza finita \( \sigma^2 \). Entonces, para \( \epsilon > 0 \) un número fijo, la probabilidad que \( X \) difiera en al menos \( \epsilon \) de su media, está acotada:
      </p>
      \[
      P(\vert X - \mu \vert \geq \epsilon ) \leq \frac{\sigma^2}{\epsilon^2}
      \]
      <p>O en términos del evento complementario: \( P(\vert X - \mu \vert < \epsilon ) \geq 1 - \frac{\sigma^2}{\epsilon^2} \).</p>
    </blockquote>
  
    <p><strong>Comentario:</strong> Este es un límite "laxo" en el sentido de que <strong>no</strong> es muy restrictivo y por tanto no muy preciso o informativo.</p>
  </section>
  
  <!-- Diapositiva 6 -->
  <section>
    <h3>Ejemplo de {-1,0,1} para Chebyshev I</h3>
  
    <blockquote>
      <p>Si \( X \) tiene tres posibles valores: \( \{-1, 0, 1\} \), con probabilidades \( \{ \frac{1}{18}, \frac{8}{9}, \frac{1}{18} \} \), respectivamente. ¿Cuál es la probabilidad \( P(\vert X - \mu \vert \geq 3\sigma) \) y cómo se compara con el límite de Chebyshev?</p>
    </blockquote>
  
    <div class="row">
      <div class="col-md-6">
        <ul>
          <li>Recordar que \( E\left[ X \right] = \sum_{i=1}^{N}x_i P(x_i) \)</li>
          <li>También que \( \sigma_{X}^{2} = E\left[ \left( X- \overline{X} \right)^2 \right] = E[X^2] - E^2[X] \)</li>
        </ul>
      </div>
      <div class="col-md-6">
        <ul>
          <li>Siendo que \( P(\vert W \vert \geq \epsilon ) \leq \frac{\sigma^2}{\epsilon^2} \)</li>
          <li>Pero una forma equivalente es \( P( \vert X - \mu_X \vert \geq k\sigma_X ) \leq \frac{1}{k^2} \)</li>
        </ul>
      </div>
    </div>
  </section>
  
  <!-- Diapositiva 7 -->
  <section>
    <h3>Ejemplo de {-1,0,1} para Chebyshev II</h3>
  
    <p>
      La media y la varianza de la variable aleatoria discreta se obtienen de la siguiente forma:
    </p>
  
    <p>
      \[
      E[X] = \sum_{i=1}^{3} x_i P(x_i) = (-1)\frac{1}{18} + (0)\frac{8}{9} + (1)\frac{1}{18} = 0
      \]
    </p>
  
    <p>
      \[
      \mathsf{Var}[X] = \sum_{i=1}^{3} (x_i - E[X])^2 P(x_i) = (-1)^2 \frac{1}{18} + (0)^2 \frac{8}{9} + (1)^2 \frac{1}{18} = \frac{1}{9}
      \]
    </p>
  
    <p>
      Utilizando la definición provista de la desigualdad de Chebyshev, se obtiene:
    </p>
  
    <p>
      \[
      P( \vert X - \mu_X \vert \geq k\sigma_X ) = P( \vert X - 0 \vert \geq 3 \frac{1}{3} ) \leq \frac{1}{3^2} = \frac{1}{9}
      \]
    </p>
  </section>
  
  <!-- Diapositiva 8 -->
  <section>
    <h3>Ejemplo de {-1,0,1} para Chebyshev III</h3>
  
    <p>
      Mientras tanto, utilizando la PDF propiamente, se puede encontrar la probabilidad \( P( \vert X \vert \geq 1 ) \) solicitada. Considerando que solo hay tres valores posibles de \( X \), \( \{-1, 0, 1\} \), los elementos de interés son \( \{-1, 1\} \) cuyas probabilidades son \( 1/18 + 1/18 = 1/9 \), igual que con Chebyshev.
    </p>
  
    <p>En general, la desigualdad de Chebyshev será mucho menos restrictiva que el análisis de la PDF, pero en este caso de ejemplo resultaron iguales.</p>
  </section>
</section>

<!-- Desigualdad de Markov -->
<section>
    <section>
        <h2>Desigualdad de Markov</h2>
    </section>

    <!-- Diapositiva 1 -->
    <section>
        <h3>Desigualdad de Markov</h3>
    
        <blockquote>
        <strong>Desigualdad de Markov</strong>
        <p>
            Si \( X \) es una variable aleatoria con \( f_X(x) = 0 \) para \( x < 0 \), entonces \( X \) es llamada una variable aleatoria no-negativa, para la cual aplica la desigualdad de Markov:
        </p>
        <p>
            \[
            P(X \geq \epsilon) \leq \frac{E[X]}{\epsilon}
            \]
        </p>
        </blockquote>
    
        <p><strong>Comentario:</strong> En contraste con el límite de Chebyshev, que involucra tanto la media como la varianza, este límite requiere únicamente de la media de \( X \).</p>
    </section>
    
    <!-- Diapositiva 2 -->
    <section>
        <h3>Prueba de la desigualdad de Markov</h3>
    
        <p>
        La consideración es ahora en relación con la definición del valor esperado (el <em>momento ordinario de primer orden</em>):
        </p>
    
        <p>
        \[
        \begin{aligned}
        E[X] = \int_{0}^{\infty} x \, f_X(x) \, \mathsf{d}x &\geq \int_{\epsilon}^{\infty} x \, f_X(x) \, \mathsf{d}x \\
        &\geq \int_{\epsilon}^{\infty} \epsilon \, f_X(x) \, \mathsf{d}x = \epsilon \int_{\epsilon}^{\infty} f_X(x) \, \mathsf{d}x \\
        &= \epsilon P(X \geq \epsilon)
        \end{aligned}
        \]
        </p>
    </section>
    
    <!-- Diapositiva 3 -->
    <section>
        <h3>Ejemplo de los resistores de baja calidad I</h3>
    
        <blockquote>
        <strong>Planteamiento</strong>
        <p>
            Es posible asumir que en la manufactura de resistores eléctricos de baja calidad de 1000 \( \Omega \), la resistencia promedio es en efecto de 1000 \( \Omega \), según se determina por un análisis estadístico de mediciones, pero hay una gran variación alrededor de este valor. Si todos los resistores por encima de 1500 \( \Omega \) deben descartarse, ¿cuál es la fracción máxima de resistores que terminarían por fuera?
        </p>
        </blockquote>
    
        <ul>
        <li>Recordar que \( P(X \geq \epsilon) \leq \frac{E[X]}{\epsilon} \)</li>
        </ul>
    </section>
    
    <!-- Diapositiva 4 -->
    <section>
        <h3>Ejemplo de los resistores de baja calidad II</h3>
    
        <p>
        En este problema tenemos una distribución no-negativa (no hay valores negativos de resistencia eléctrica), y disponemos únicamente de la media y no de la varianza de la distribución, entonces aplica la desigualdad de Markov.
        </p>
    
        <p>
        \[
        P(X \geq \epsilon) = P(X \geq 1500) \leq \frac{E[X]}{\epsilon} = \frac{1000}{1500} = \frac{2}{3}
        \]
        </p>
    
        <p>
        Es posible, aun así, que la proporción de resistencias descartadas sea menor. Bastaría con conocer la distribución para tener más certeza.
        </p>
    </section>
</section>

<!--Ley de los grandes números-->
<section>
    <section>
        <h2>Ley de los grandes números I</h2>
    </section>

    <!-- Diapositiva 1 -->
    <section>
        <h3>Ley de los grandes números I</h3>
    
        <p>
        Sea \( \{X_i\}_{i=1}^N \) una muestra aleatoria. Los \( X_i \) son variables aleatorias idénticamente distribuidas e independientes IDD con media común \( \mu \) y varianza \( \sigma^2 \). Se espera intuitivamente que la media de la muestra, \( \overline{X_N}, \) debería ser cercana a la media de la población \( \mu \) para \( N \) grande. Esto es:
        </p>
    
        <p>
        \[
        \lim_{N \rightarrow \infty} \overline{X_N} = \lim_{N \rightarrow \infty} \frac{S_N}{N} = \mu
        \]
        </p>
    
        <p>donde \( S_N = X_1 + \ldots + X_N \) es la suma.</p>
    </section>
    
    <!-- Diapositiva 2 -->
    <section>
        <h3>Ley de los grandes números II</h3>
    
        <p>
        Sea \( \epsilon > 0 \) un número fijo. Por la independencia de la secuencia \( X_1, \ldots, X_N \) en la muestra aleatoria, se cumple que:
        </p>
    
        <p>
        \[
        E[S_N] = N\mu \qquad \sigma_{S_N}^2 = N\sigma^2 
        \]
        </p>
    
        <p>
        La desigualdad de Chebyshev aplicada a \( S_N \) establece que:
        </p>
    
        <p>
        \[
        P(\vert S_N - N\mu \vert \geq N\epsilon ) \leq \frac{\sigma_{S_N}^2}{(N\epsilon)^2} = \frac{N\sigma^2}{N^2\epsilon^2} = \frac{\sigma^2}{N\epsilon^2}
        \]
        </p>
    
        <p>En términos de la media \( \overline{X_N} \) de la muestra:</p>
    
        <p>
        \[
        P(\vert \overline{X_N} - \mu \vert \geq \epsilon ) = P\left( \left\vert \frac{S_N}{N} - \mu \right\vert \geq \epsilon  \right) = P(\vert S_N - N\mu \vert \geq N\epsilon ) \leq \frac{\sigma^2}{N\epsilon^2}
        \]
        </p>
    
        <p>
        Para \( N \rightarrow \infty \), la cota de la derecha en la última ecuación tiende a 0 y entonces:
        </p>
    </section>
    
    <!-- Diapositiva 3 -->
    <section>
        <h3>Ley de los grandes números III</h3>
    
        <blockquote>
        <strong>Ley de los grandes números</strong>
        <p>
            Sea \( \{ X_i \}_{i=1}^{N} \) una muestra aleatoria con media común \( \mu \) y varianza \( \sigma^2 \). Sea:
        </p>
        <p>
            \[
            S_N = X_1 + \cdots + X_N  
            \]
        </p>
        <p>
            Entonces:
        </p>
        <p>
            \[
            P\left( \left\vert \frac{S_N}{N} - \mu \right\vert \geq \epsilon \right) \rightarrow 0
            \]
        </p>
        <p>conforme \( N \rightarrow \infty \) para cualquier \( \epsilon > 0 \).</p>
        </blockquote>
    </section>
    
    <!-- Diapositiva 4 -->
    <section>
        <h3>Del rigor en la ciencia</h3>
        <p><em>Jorge Luis Borges</em></p>
    
        <p>
        En aquel Imperio, el arte de la cartografía logró tal perfección que el mapa de una sola provincia ocupaba toda una ciudad, y el mapa del Imperio, toda una provincia. Con el tiempo, estos mapas desmesurados no satisficieron y los colegios de cartógrafos levantaron un mapa del Imperio, que tenía el tamaño del Imperio y coincidía puntualmente con él.
        </p>
    
        <p>
        Menos adictas al estudio de la cartografía, las generaciones siguientes entendieron que ese dilatado mapa era inútil y no sin impiedad lo entregaron a las inclemencias del sol y los inviernos. En los desiertos del oeste perduran despedazadas ruinas del mapa, habitadas por animales y por mendigos; en todo el país no hay otra reliquia de las disciplinas geográficas.
        </p>
    </section>
</section>


<!-- Gráficos de Chart.js -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/PapaParse/5.4.1/papaparse.min.js"></script>
<script src="{% static 'js/p15.js' %}"></script>

{% endblock %}

{% block websocket %}
{% if role == "presenter" %}
<script>
    const wsRouteTx = 'wss://' + window.location.host + '/ws/deck/slider/p4/';
    const deckSocket = new WebSocket(wsRouteTx);

    // Registrar la conexión abierta
    deckSocket.onopen = function (event) {
        console.log('Conexión WebSocket abierta exitosamente como presentador en ' + wsRouteTx);
    };

    Reveal.on('slidechanged', event => {
        deckSocket.send(JSON.stringify({
            'indexh': event.indexh,
            'indexv': event.indexv
        }));
    });
</script>
{% else %}
<script>
    const wsRouteRx = 'wss://' + window.location.host + '/ws/deck/slider/p4/';
    const deckSocket = new WebSocket(wsRouteRx);

    // Registrar la conexión abierta
    deckSocket.onopen = function (event) {
        console.log('Conexión WebSocket abierta exitosamente como cliente en ' + wsRouteRx);
    };

    deckSocket.onmessage = function (event) {
        const data = JSON.parse(event.data);
        Reveal.slide(data.message.indexh, data.message.indexv);
    };
</script>
{% endif %}
{% endblock %}